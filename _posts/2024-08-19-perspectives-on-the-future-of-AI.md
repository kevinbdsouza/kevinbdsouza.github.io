---
title: 'Perspectives on the future of AI'
date: 2024-08-19
permalink: /posts/2024/08/perspectives-on-the-future-of-AI
tags:
  - artificial intelligence 
  - generalization 
  - perspective 
---

How big are the models going to get and how much longer is the scaling hypothesis going to hold? It's unclear, but according to current performance trends, which haven't shown signs of plateauing (GPT-4o, Claude 3.5 Sonnet, Gemini-1.5-Pro, Llama-3.1-405B, Grok-2), and the power budget of announced data centres (5GW OpenAI/Microsoft Stargate campus), it is likely that there is an order of magnitude left (OOM) to climb in model size. This <a href="https://epochai.org/blog/can-ai-scaling-continue-through-2030"><u>Epoch AI research</u></a> covers these scenarios in depth and estimates training runs of the order of ~2e29 FLOPs being possible by 2030, which would be 4 OOMs larger than GPT-4 (2e25 FLOPs). These training runs will primarily be power constrained, followed by chips, data, and latency. 

Where is this additional energy going to come from? As these compute centres need baseload, will new colocalized natural gas, hydroelectric power plants be built, can solar+wind power plus storage deliver part of this, will new energy sources like enhanced geothermal be part of the equation? These questions remain to be answered, however, in absence of hail mary's like drastically new forms of energy, the additional energy has to come from a mix of these sources. It is unlikely that nuclear power plants can be commissioned and built in the time frame relevant here. An alternate route is to perform geographically distributed training, which would also allow training runs with an even larger power budget. Some of the frontier models like Gemini may already be trained this way. Recent research by Nous Research like <a href="https://raw.githubusercontent.com/NousResearch/DisTrO/main/A_Preliminary_Report_on_DisTrO.pdf"><u>DisTrO</u></a> shows that it is possible to design optimizers that reduce the inter-GPU communication by 4-5 OOMs, powering low-latency training of large models on sluggish internet bandwidths. So the future may involve completely geographically distributed training, as a work around for power constraints. 

The power of synthetic data has become obvious in the past year, with the <a href="https://arxiv.org/abs/2407.21783"><u>Llama-3.1 technical report</u></a> making multiple references to it and showing how it can contribute in a big way to post-training including supervised fine-tuning and task-specific capabilities such as coding, long-context reasoning, tool use, multilinguality etc. The Llama-3.1 technical report is a goldmine of useful information which will take months to fully digest and will undoubtedly go down as one of the best open source contributions to be made, highly recommend checking parts of it out. It seems that synthetic data generated from <a href="https://www.arxiv.org/abs/2408.16737"><u>weaker but cheaper models</u></a> may actually be compute optimal compared to stronger and more expensive models. Grounded synthetic data generation may have a role to play in generating chain of thought trajectories as well, which can be used for bootsrapping in-context learning (ICL). 

ICL has been a revelation in the past couple of years, with it being able to unlock capabilities during inference time. Even though the word "agent" is abused by the AI influencer class, ICL does enable agentic behaviour by providing power ups in reasoning (few shot examples), tool use, and task-specific retrieval (<a href="https://arxiv.org/abs/2005.11401"><u>retrieval augmented generation (RAG)</u></a>, <a href="https://arxiv.org/abs/2404.16130"><u>Graph RAG</u></a>). <a href="https://github.com/cohere-ai/cohere-toolkit"><u>Cohere</u></a> seems to be going all out on this and using it to build AI for enterprise applications. It doesn't stop there. With prompt engineering becoming a real thing (magic prompts do exist such as the <a href="https://github.com/NeoVertex1/SuperPrompt"><u>SuperPrompt</u></a>), it will increasingly become a skill to elicit enhanced behaviour from the models. For a detailed survey of all the prompting techniques see the <a href="https://trigaten.github.io/Prompt_Survey_Site"><u>Prompt Report</u></a>. Prompting is not just about finding magic words but also baking in reasoning techniques in context, such as <a href="https://arxiv.org/abs/2201.11903"><u>Chain-of-Thought</u></a>, <a href="https://arxiv.org/abs/2203.11171"><u>Self-Consistency</u></a>, <a href="https://arxiv.org/abs/2303.17651"><u>Self-Refine</u></a>, <a href="https://arxiv.org/abs/2212.09561"><u>Self-Verification</u></a>, <a href="https://arxiv.org/abs/2405.06682"><u>Self-Reflection</u></a>, <a href="https://arxiv.org/abs/2305.10601"><u>Tree of Thoughts</u></a>, <a href="https://arxiv.org/abs/2308.09687"><u>Graph of Thoughts</u></a>, <a href="https://arxiv.org/abs/2403.09629"><u>Quiet-STaR</u></a>, <a href="https://arxiv.org/abs/2303.11366"><u>Reflexion</u></a>, and many others. Some methods even try to automatically create agentic system designs, including coming up with novel building blocks and integrating them such as <a href="https://arxiv.org/pdf/2408.08435"><u>ADAS</u></a>. ADAS shows that a meta agent can be used to program new agents in code. Interfacing everything in code, seems like a promising way forward for agentic systems. 

Google's Gemini seems to be boasting longer and longer context lengths and their <a href="https://arxiv.org/abs/2403.05530"><u>technical report</u></a> claims near-perfect retrieval up to at least 10M tokens. Such long context reasoning could be powerful when handling multiple long documents, entire codebases, and multimodal inputs. Such long context could potentially be achieved by modified attention mechanisms such as <a href="https://arxiv.org/abs/2404.07143"><u>Infini-attention</u></a> and <a href="https://arxiv.org/abs/2310.01889"><u>Ring Attention</u></a>. <a href="https://arxiv.org/abs/2404.11018"><u>Many-Shot ICL (MICL)</u></a> has been shown to provide substantial performance gains over few-shot ICL and long context maybe the enabler. Further, long contexts can enable elaborate deliberate planning during reasoning, such as <a href="https://arxiv.org/abs/2409.03733"><u>PlanSearch</u></a>. Chain-of-Thought (COT), in particular, has been shown to unlock the ability to perform <a href="https://arxiv.org/abs/2402.12875"><u>serial computation</u></a>, which would othwerise be outside the region of expressiveness of transformers. What happens when you combine multiple of these agents with different base models and leverage their collective strengths to problem solving? You get <a href="https://arxiv.org/abs/2406.04692"><u>Mixture-of-Agents (MoA)</u></a> as demonstrated by Together AI and trade-off latency for performance gains.       

The great thing about agentic systems and ICL is that they unlock an orthogonal dimension for capability enhancement, when compared to pre-training and post-training. Another orthogonal dimension is unlocked by inference time compute capability. As this <a href="https://epochai.org/blog/trading-off-compute-in-training-and-inference"><u>Epoch AI research</u></a> estimates that it may be possible to increase inference time compute by 1-2 OOMs to save ~1 OOM in training compute. Performance improvement with <a href="https://arxiv.org/abs/2407.21787"><u>repeated sampling</u></a> and <a href="https://arxiv.org/abs/2408.03314"><u>optimal test-time compute allocation</u></a> has been recently demonstrated. This would be a direction that big labs will increasingly pursue not just because it could potentially offer a way to do deliberate search during inference and enhance reasoning skills, but also because it would offer a way to offload the compute to the user during inference and make the user pay for it instead. This is already vindicated by the release of OpenAI o1-preview and o1-mini, and substantial improvements seen in multiple independent reasoning benchmarks. This superior performance comes at the cost of latency and to some extent brittleness. It seems that it is much easier to converse, interrupt, and steer a model like Claude 3.5 sonnet than it is steer o1 because it gets too entangled in its own COT and can't maneuver effectively. Regarding whether this is a good approach both strategically and technically for general purpose reasoning problems remains to be seen in the months to come. 

Transformers by themselves have shown to reduce multi-step compositional reasoning into linearized subgraph matching (<a href="https://arxiv.org/abs/2305.18654"><u>Faith and Fate</u></a>), performing <a href="https://www.answer.ai/posts/2024-07-25-transformers-as-matchers.html"><u>fuzzy pattern matching</u></a>. Therefore, without deliberate planning, COT, and search, reasoning maybe out of reach. A surprsing finding is that reinforcement learning (RL) and search maybe effective at not just domains where outputs can be formally verified such as math and coding, but also for general purpose reasoning. To some extent, the benefits of adding search to LLMs was out in the open (<a href="https://yellow-apartment-148.notion.site/AI-Search-The-Bitter-er-Lesson-44c11acd27294f4495c3de778cd09c8d"><u>The Bitter-er Lesson</u></a>). The initial findings from o1 in this direction are indeed promising but not yet fully convincing. The precedent for superhuman RL comes of course from Go, with <a href="https://www.nature.com/articles/nature16961"><u>AlphaGo</u></a> using Monte Carlo Tree Search (MCTS) from DeepMind. This was later further generalized to multiple games by them in <a href="https://arxiv.org/abs/1712.01815"><u>AlphaZero</u></a> and then extended to learn without any knowledge of underlying dynamics in <a href="https://www.nature.com/articles/s41586-020-03051-4"><u>MuZero</u></a>. Whether MCTS is powerful enough to span the general purpose reasoning space in a granular manner remains to be seen, but it would require a great policy model (which the frontier LLMs already are), and a reward model that can perform reliable process supervision (Process Reward Models (PRMs)). Process supervision has been shown to provide gains in reasoning such as in <a href="https://arxiv.org/abs/2305.20050"><u>Let's Verify Step by Step</u></a>, <a href="https://arxiv.org/abs/2405.00451"><u>Iterative Preference Learning</u></a>, <a href="https://arxiv.org/abs/2406.03816"><u>ReST-MCTS\*</u></a>, <a href="https://arxiv.org/abs/2409.08642"><u>Critical Planning</u></a>, <a href="https://arxiv.org/abs/2305.14992"><u>Reasoning via Planning</u></a>, <a href="https://arxiv.org/abs/2406.14283"><u>Q\*</u></a>, <a href="https://arxiv.org/abs/2406.06592"><u>Automated Process Supervision</u></a>, and <a href="https://arxiv.org/abs/2405.03553"><u>AlphaMath Almost Zero</u></a>, and most of these methods use the MCTS framework to learn step-level rewards and perform search. To what extent techniques like <a href="https://arxiv.org/abs/2205.10816"><u>procedural cloning</u></a> can be a substitute for process supervision is unclear.  

As shown by many of these methods, math and code seem to be the low hanging fruits in reasoning, with our ability to get explicit rewards for problems in these domains. Progress in these domains has been demonstrated by multiple players recently, including DeepSeek with its <a href="https://www.arxiv.org/abs/2408.08152"><u>DeepSeek-Prover-V1.5</u></a> and <a href="https://arxiv.org/abs/2406.11931"><u>DeepSeek-Coder-V2</u></a>. The whale has some incredibly talented people and its open source contributions in these domains is nothing short of game changing. There has been other evidence of progress in math being possible with DeepMind's <a href="https://deepmind.google/discover/blog/ai-solves-imo-problems-at-silver-medal-level"><u>AlphaProof</u></a> and <a href="https://www.nature.com/articles/s41586-023-06747-5"><u>AlphaGeometry</u></a> achieving a silver-medal level performance in the International Mathematical Olympiad (IMO). This combined with impressive performances in competitions such as the <a href="https://www.kaggle.com/competitions/ai-mathematical-olympiad-prize"><u>AI Mathematical Olympiad (AIMO)</u></a> by projects like <a href="https://github.com/project-numina/aimo-progress-prize"><u>Numina</u></a>, which finetuned <a href="https://huggingface.co/deepseek-ai/deepseek-math-7b-base"><u>DeepSeekMath-Base 7B</u></a> is quite impressive to say the least. This could be foreseen for a while because of the ability of a formal language like <a href="https://lean-lang.org"><u>Lean</u></a> to act as a verifier and introduction of projects like <a href="https://github.com/lean-dojo/LeanCopilot"><u>LeanCopilot</u></a> using LLMs in the loop for theorem proving.

Architectural and algorithmic improvements could hold the key to unlock the next generation of AI systems. Will transformers rule till the end or do newer architectures like <a href="https://arxiv.org/abs/2312.00752"><u>Mamba</u></a> and <a href="https://arxiv.org/abs/2405.04517"><u>xLSTM</u></a> have a chance? At this moment, doesn't look like it, but things may change. However, there have been models that activate a subset of parameters for each token (<a href="https://arxiv.org/abs/2406.18219"><u>Mixture-of-experts (MoE)</u></a>), and such models may exhibit distinct behaviour when compared to models that don't employ MOE. Which is better remains uncertain, but MOE does allow you to decouple model size from computational cost, potentially enabling utilization of a massive number of experts (<a href="https://arxiv.org/abs/2407.04153"><u>Mixture of A Million Experts</u></a>) and unlocking the next OOM of transformer scaling. Routing can also be done at the token level and FLOPs can be dynamically allocated  across model depths (<a href="https://arxiv.org/abs/2404.02258"><u>Mixture-of-Depths</u></a>), allowing certain tokens to use more compute than others. This is going to be crucial going forward to enable inference time compute scaling, as not all tokens need long compute paths. Better optimizers such as <a href="https://arxiv.org/pdf/2002.09018"><u>Scaling Shampoo</u></a> and <a href="https://arxiv.org/abs/2409.03137"><u>AdEMAMix</u></a>, better sampling methods such as <a href="https://arxiv.org/abs/2407.01082"><u>Min P</u></a>, and better gradient manipulation during optimization such as <a href="https://arxiv.org/abs/2405.20233"><u>Grokfast</u></a> may provide meaningful gains for the next geneartion of models. 

Intepretability has seen some significant progress in the past few years mainly driven by work at Anthropic. Basically everything on <a href="https://www.anthropic.com/research#interpretability"><u>interpretability</u></a> by Anthropic is worth checking out, particularly <a href="https://transformer-circuits.pub/2021/framework/index.html"><u>Transformer Circuits</u></a>, <a href="https://transformer-circuits.pub/2022/in-context-learning-and-induction-heads/index.html"><u>Induction Heads</u></a>, <a href="https://transformer-circuits.pub/2022/toy_model/index.html"><u>Superposition</u></a>, and <a href="https://transformer-circuits.pub/2023/monosemantic-features"><u>Monosemanticity</u></a>. The evidence that interpretable features occur as a superposition of multiple neurons and that with <a href="https://arxiv.org/abs/2309.08600"><u>sparse autoencoders (SAEs)</u></a> we can arrive at learned features that are more monosemantic in nature. SAEs are an unsupervised method for learning a sparse decomposition of a neural network’s latent representations. With the <a href="https://transformer-circuits.pub/2024/scaling-monosemanticity/"><u>scaling of SAEs</u></a> to bigger models like Claude 3 Sonnet and the release of open source tools like <a href="https://arxiv.org/abs/2408.05147"><u>Gemma Scope</u></a> and <a href="https://github.com/jbloomAus/SAELens"><u>SAELens</u></a>, this direction looks very promising for steering LLMs (<a href="https://arxiv.org/abs/2308.10248"><u>steering vectors</u></a>, <a href="https://www.alignmentforum.org/posts/C5KAZQib3bzzpeyrg/progress-update-1"><u>feature steering</u></a>, <a href="https://transformer-circuits.pub/2024/scaling-monosemanticity/index.html"><u>clamping</u></a>). There are other interpretability studies that <a href="https://www.arxiv.org/abs/2408.16293"><u>probe network behaviour with synthetic data</u></a>, which are worth looking at as well.                   

Smaller models are getting better. 8B models are reaching performance levels previously only achievable by models OOM bigger, mainly through power of techniques like pruning and distillation. The <a href="https://www.arxiv.org/abs/2407.14679"><u>Minitron family of models</u></a> by Nividia and the accompanying paper are a great source detailing these techniques. We will be able to cram more and more performance in smaller models, and it remains to be seen how small can we go and achieve reliable performance on the edge. Quantizating models from FP16 to FP8 and FP4, also provides considerable speedups that enables inference on compute and memory constrained applications. Moreover, binary (<a href="https://arxiv.org/abs/2310.11453"><u>BitNet</u></a>) and ternary quantization (<a href="https://arxiv.org/abs/2402.17764"><u>BitNet b1.58</u></a>, <a href="https://arxiv.org/abs/2406.02528"><u>MatMul-free</u></a>) maybe viable as well, potentially leading to matrix multiplication free accelerators. Any conversation about AI is incomplete without talking about hardware. "During a gold rush, sell shovels" they say, and boy would that have been perfect advice when AI was taking off, because NVIDIA's growth over the past couple of decades has been astronomical. Will Nvidia stay in the lead with its CUDA ecosystem and continuously improving GPUs (<a href="https://nvidianews.nvidia.com/news/nvidia-blackwell-platform-arrives-to-power-a-new-era-of-computing"><u>Blackwell</u></a>), or hardware providers like <a href="https://groq.com/"><u>Groq</u></a> and <a href="https://cerebras.ai/"><u>Cerebras</u></a> take over the inference space with their custom hardware? Could be totally possible. The complete dynamics between Google's TPU ecosystem, Meta, Apple, OpenAI, and Microsoft would be hard to predict, but would be surprsing if Nvidia doesn't maintain its lead in the hardware space. 

I haven't touched upon so many relevant things here, such as developments in image, video, voice, time-series, multi-modality, 3D, robotics, biology, climate, and many others. The applications in all these domains are equally exciting and the pace of progress doesn't show signs of slowing. Robotics, for instance, is poised to take over the world in the next decade. Every home may have a personal robot that is good at a variety of tasks. Biology promises to model molecules and their interactions in a fine grained way to tackle various diseases, allowing us to tame biological complexity. Smaller models maybe pervasive once they are plugged into inference time resoning chains. These could all be positive things, but they may not be. Technology is always a double edged sword. A world of sheer abundance or a world of stark inequality? We should not take the enormity of this moment for granted.     
